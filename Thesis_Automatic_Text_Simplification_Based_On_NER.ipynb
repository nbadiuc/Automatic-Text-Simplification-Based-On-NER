{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbadiuc/Automatic-Text-Simplification-Based-On-NER/blob/main/Thesis_Automatic_Text_Simplification_Based_On_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installations and imports"
      ],
      "metadata": {
        "id": "aIhi3lYiHuDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install xformers\n",
        "!pip install sentencepiece\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "0NDAXzj7FsLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062b715b-1f36-470b-d126-791cfeb544bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.22.4)\n",
            "Requirement already satisfied: pyre-extensions==0.0.29 in /usr/local/lib/python3.10/dist-packages (from xformers) (0.0.29)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: typing-inspect in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->xformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->xformers) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect->pyre-extensions==0.0.29->xformers) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.0.53)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "import string\n",
        "import requests\n",
        "import json\n",
        "import random\n",
        "import re"
      ],
      "metadata": {
        "id": "Yi_WnMMtIBOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9z70zSKILaq",
        "outputId": "82e8ba7e-73b1-495b-e60b-3c2962dfeafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DwVwsOXIzpa",
        "outputId": "0c37a3fa-c00a-4c83-831c-6fcc392872e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAcg5zU-lPKa",
        "outputId": "79221524-efd2-4b23-8847-ef1675203f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The baseline model"
      ],
      "metadata": {
        "id": "iaq9JrhRKCQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_tokenizer = AutoTokenizer.from_pretrained(\"orzhan/rugpt3-simplify-large\", padding_side='left')\n",
        "baseline_model = AutoModelForCausalLM.from_pretrained(\"orzhan/rugpt3-simplify-large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt2EYncaF0cz",
        "outputId": "0fbfcec9-9536-4720-dce3-5b1c8fae8740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_simplifier = pipeline(\"text-generation\", model=baseline_model, tokenizer=baseline_tokenizer, max_length=150)"
      ],
      "metadata": {
        "id": "kNBcQY6_7l3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def base_model(sentence):\n",
        "    sentence = \"<s>\" + sentence + \"<Simplify:>\"\n",
        "    while sentence.find(\"EndText\") == -1:\n",
        "        sentence = baseline_simplifier(sentence)[0]['generated_text']\n",
        "    return sentence[sentence.find(\"<Simplify:>\")+12:sentence.find(\"EndText\")-1]"
      ],
      "metadata": {
        "id": "kzKFdlfOEf7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(base_model(\"Финансовым директором социальной сети Facebook назначен 39-летний Дэвид Эберсман (David Ebersman), сообщает The Wall Street Journal.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zm6DH6FFqhk",
        "outputId": "07d31d32-7af4-4df8-df95-3b728bb97860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Дэвид Эберсман (David Ebersman) - финансовый директор социальной сети Facebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT and synonym dictionary models"
      ],
      "metadata": {
        "id": "ennAbE7z7cHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset into the /tmp folder\n",
        "!wget \"https://github.com/nerel-ds/NEREL/archive/refs/heads/master.zip\" -O \"/tmp/nerel.zip\"\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/tmp/nerel.zip', 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NDINHVXCxEp",
        "outputId": "feb08d96-dfb0-4534-e63d-d7b7a0147383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-04 08:42:51--  https://github.com/nerel-ds/NEREL/archive/refs/heads/master.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/nerel-ds/NEREL/zip/refs/heads/master [following]\n",
            "--2023-06-04 08:42:51--  https://codeload.github.com/nerel-ds/NEREL/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 192.30.255.120\n",
            "Connecting to codeload.github.com (codeload.github.com)|192.30.255.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘/tmp/nerel.zip’\n",
            "\n",
            "/tmp/nerel.zip          [    <=>             ]   6.14M  8.14MB/s    in 0.8s    \n",
            "\n",
            "2023-06-04 08:42:52 (8.14 MB/s) - ‘/tmp/nerel.zip’ saved [6443546]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading a text and its annotation\n",
        "def read_text(text_location=\"/tmp/NEREL-master/NEREL-v1.1/train/023.txt\"):\n",
        "\n",
        "    with open(text_location) as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    # getting rid of the titles of the articles - they are not full sentences\n",
        "    offset = len(lines[0]) # title length\n",
        "\n",
        "    text = ''\n",
        "    for i in range(1, len(lines)):\n",
        "        text += lines[i]\n",
        "\n",
        "    ann_location = text_location[:-3] + \"ann\"\n",
        "    with open(ann_location) as f:\n",
        "        ann = f.readlines()\n",
        "\n",
        "    return text, ann, offset"
      ],
      "metadata": {
        "id": "S05fdmDaY7kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text, ann, offset = read_text(text_location=\"/tmp/NEREL-master/NEREL-v1.1/train/023.txt\")\n",
        "print(\"Text:\")\n",
        "print(text)\n",
        "print(\"Annotation:\")\n",
        "print(ann)\n",
        "print(\"Offset:\")\n",
        "print(offset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I043eNDcb4yv",
        "outputId": "46e6ebd4-3e38-4bfc-eeff-d9319d44b8fd"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:\n",
            "Финансовым директором социальной сети Facebook назначен 39-летний Дэвид Эберсман (David Ebersman), сообщает The Wall Street Journal. На работу в Facebook он выйдет в сентябре. Ранее Эберсман был финансовым директором биотехнологической компании Genentech.\n",
            "\n",
            "Эберсман подчеркнул, что видит много общего между Facebook и Genentech. В частности, это две быстрорастущие компании с сильной корпоративной культурой. Также он заявил, что Facebook ожидает 70-процентное увеличение выручки в 2009 году.\n",
            "\n",
            "В компании Genentech Дэвид Эберсман проработал 15 лет. Ее финансовым директором он стал в 2006 году. На этой должности Эберсман проработал до апреля 2009 года, когда Roche Holding купил Genentech.\n",
            "\n",
            "По данным The Wall Street Journal, на должность финансового директора Facebook претендовали 11 кандидатов. Каждый из них прошел собеседование с основателем Facebook Марком Цукербергом (Mark Zuckerberg) и другими руководителями компании.\n",
            "\n",
            "Ранее финансовым директором Facebook был Гидеон Ю (Gideon Yu). Он покинул компанию три месяца назад. Вместо него социальная сеть решила нанять топ-менеджера с опытом работы в публичной компании.\n",
            "\n",
            "Facebook является крупнейшей западной социальной сетью. За пять лет существования этого проекта его аудитория превысила 200 миллионов человек. \n",
            "Annotation:\n",
            "['T1\\tORGANIZATION 0 8\\tFacebook\\n', 'T2\\tPROFESSION 22 43\\tфинансового директора\\n', 'T3\\tPROFESSION 44 65\\tФинансовым директором\\n', 'T4\\tORGANIZATION 82 90\\tFacebook\\n', 'T5\\tAGE 100 109\\t39-летний\\n', 'T6\\tPERSON 110 124\\tДэвид Эберсман\\n', 'T7\\tPERSON 126 140\\tDavid Ebersman\\n', 'T8\\tORGANIZATION 152 175\\tThe Wall Street Journal\\n', 'T9\\tORGANIZATION 189 197\\tFacebook\\n', 'T10\\tDATE 208 218\\tв сентябре\\n', 'T11\\tPERSON 226 234\\tЭберсман\\n', 'T12\\tPROFESSION 239 260\\tфинансовым директором\\n', 'T13\\tORGANIZATION 289 298\\tGenentech\\n', 'T14\\tPERSON 301 309\\tЭберсман\\n', 'T15\\tORGANIZATION 351 359\\tFacebook\\n', 'T16\\tORGANIZATION 362 371\\tGenentech\\n', 'T17\\tNUMBER 390 393\\tдве\\n', 'T18\\tORGANIZATION 474 482\\tFacebook\\n', 'T19\\tPERCENT 491 504\\t70-процентное\\n', 'T20\\tDATE 524 535\\tв 2009 году\\n', 'T21\\tORGANIZATION 549 558\\tGenentech\\n', 'T22\\tPERSON 559 573\\tДэвид Эберсман\\n', 'T23\\tDATE 585 591\\t15 лет\\n', 'T24\\tPROFESSION 596 617\\tфинансовым директором\\n', 'T25\\tDATE 626 637\\tв 2006 году\\n', 'T26\\tPERSON 657 665\\tЭберсман\\n', 'T27\\tDATE 677 696\\tдо апреля 2009 года\\n', 'T28\\tORGANIZATION 704 717\\tRoche Holding\\n', 'T29\\tORGANIZATION 724 733\\tGenentech\\n', 'T30\\tORGANIZATION 746 769\\tThe Wall Street Journal\\n', 'T31\\tPROFESSION 784 805\\tфинансового директора\\n', 'T32\\tNUMBER 828 830\\t11\\n', 'T34\\tPERSON 901 919\\tМарком Цукербергом\\n', 'T35\\tPERSON 921 936\\tMark Zuckerberg\\n', 'T36\\tPROFESSION 980 1001\\tфинансовым директором\\n', 'T37\\tPERSON 1015 1023\\tГидеон Ю\\n', 'T38\\tPERSON 1025 1034\\tGideon Yu\\n', 'T39\\tDATE 1057 1073\\tтри месяца назад\\n', 'T40\\tPROFESSION 1117 1130\\tтоп-менеджера\\n', 'T41\\tORGANIZATION 1170 1178\\tFacebook\\n', 'T42\\tDATE 1226 1237\\tЗа пять лет\\n', 'T43\\tNUMBER 1290 1303\\t200 миллионов\\n', 'T47\\tPROFESSION 784 814\\tфинансового директора Facebook\\n', 'T48\\tORGANIZATION 806 814\\tFacebook\\n', 'T33\\tORGANIZATION 892 900\\tFacebook\\n', 'T49\\tPROFESSION 948 971\\tруководителями компании\\n', 'T50\\tORGANIZATION 1002 1010\\tFacebook\\n', 'T52\\tPROFESSION 980 1010\\tфинансовым директором Facebook\\n', 'T53\\tLOCATION 1199 1207\\tзападной\\n', 'R1\\tAGE_IS Arg1:T6 Arg2:T5\\t\\n', 'R2\\tALTERNATIVE_NAME Arg1:T6 Arg2:T7\\t\\n', 'R3\\tWORKPLACE Arg1:T6 Arg2:T4\\t\\n', 'R4\\tWORKS_AS Arg1:T6 Arg2:T3\\t\\n', 'R5\\tWORKPLACE Arg1:T22 Arg2:T21\\t\\n', 'R6\\tWORKS_AS Arg1:T22 Arg2:T24\\t\\n', 'R7\\tALTERNATIVE_NAME Arg1:T34 Arg2:T35\\t\\n', 'R8\\tFOUNDED_BY Arg1:T33 Arg2:T34\\t\\n', 'R9\\tALTERNATIVE_NAME Arg1:T37 Arg2:T38\\t\\n', 'R10\\tWORKS_AS Arg1:T37 Arg2:T52\\t\\n', 'R11\\tWORKPLACE Arg1:T37 Arg2:T50\\t\\n', 'R12\\tWORKPLACE Arg1:T2 Arg2:T1\\t\\n', 'T44\\tEVENT 91 99\\tназначен\\n', 'R13\\tWORKPLACE Arg1:T3 Arg2:T4\\t\\n', 'R14\\tPARTICIPANT_IN Arg1:T4 Arg2:T44\\t\\n', 'R15\\tPARTICIPANT_IN Arg1:T6 Arg2:T44\\t\\n', 'R16\\tWORKPLACE Arg1:T11 Arg2:T13\\t\\n', 'R17\\tWORKS_AS Arg1:T11 Arg2:T12\\t\\n', 'R18\\tWORKPLACE Arg1:T12 Arg2:T13\\t\\n', 'R19\\tALTERNATIVE_NAME Arg1:T6 Arg2:T11\\t\\n', 'R20\\tOWNER_OF Arg1:T28 Arg2:T29\\t\\n', 'R21\\tWORKPLACE Arg1:T47 Arg2:T48\\t\\n', 'R22\\tWORKPLACE Arg1:T52 Arg2:T50\\t\\n', 'R23\\tALTERNATIVE_NAME Arg1:T36 Arg2:T40\\t\\n', 'R24\\tHEADQUARTERED_IN Arg1:T41 Arg2:T53\\t\\n', 'T45\\tEVENT 718 723\\tкупил\\n', 'R25\\tAGENT Arg1:T28 Arg2:T45\\t\\n', 'R26\\tPARTICIPANT_IN Arg1:T29 Arg2:T45\\t\\n', 'R27\\tWORKPLACE Arg1:T26 Arg2:T29\\t\\n', 'R28\\tPOINT_IN_TIME Arg1:T45 Arg2:T27\\t\\n', 'R29\\tWORKS_AS Arg1:T34 Arg2:T49\\t\\n', 'R30\\tWORKPLACE Arg1:T49 Arg2:T33\\t\\n', 'T46\\tEVENT 1040 1056\\tпокинул компанию\\n', 'R31\\tPARTICIPANT_IN Arg1:T50 Arg2:T46\\t\\n', 'R32\\tAGENT Arg1:T37 Arg2:T46\\t\\n', 'R33\\tPOINT_IN_TIME Arg1:T46 Arg2:T39\\t\\n', 'N1\\tReference T13 Wikidata:Q899140\\tGenentech\\n', 'N2\\tReference T26 Wikidata:Q5233251\\tДэвид Эберсман\\n', 'N3\\tReference T30 Wikidata:Q164746\\tThe Wall Street Journal\\n', 'N8\\tReference T48 Wikidata:Q355\\tFacebook\\n', 'N9\\tReference T49 Wikidata:Q1162163\\tдиректор\\n', 'N11\\tReference T28 Wikidata:NULL\\tнет данных\\n', 'N7\\tReference T47 Wikidata:NULL\\tнет данных\\n', 'N4\\tReference T35 Wikidata:Q36215\\tМарк Цукерберг\\n', 'N5\\tReference T37 Wikidata:NULL\\tнет данных\\n', 'N6\\tReference T40 Wikidata:Q2563977\\t\\n', 'N10\\tReference T53 Wikidata:Q679\\tзапад\\n', 'N12\\tReference T29 Wikidata:Q899140\\tGenentech\\n', 'N13\\tReference T21 Wikidata:Q899140\\tGenentech\\n', 'N14\\tReference T16 Wikidata:Q899140\\tGenentech\\n', 'N15\\tReference T14 Wikidata:Q5233251\\tДэвид Эберсман\\n', 'N16\\tReference T11 Wikidata:Q5233251\\tДэвид Эберсман\\n', 'N17\\tReference T7 Wikidata:Q5233251\\tДэвид Эберсман\\n', 'N18\\tReference T22 Wikidata:Q5233251\\tДэвид Эберсман\\n', 'N19\\tReference T6 Wikidata:Q5233251\\tДэвид Эберсман\\n', 'N20\\tReference T8 Wikidata:Q164746\\tThe Wall Street Journal\\n', 'N21\\tReference T4 Wikidata:Q355\\tFacebook\\n', 'N22\\tReference T9 Wikidata:Q355\\tFacebook\\n', 'N23\\tReference T15 Wikidata:Q355\\tFacebook\\n', 'N24\\tReference T41 Wikidata:Q355\\tFacebook\\n', 'N25\\tReference T33 Wikidata:Q355\\tFacebook\\n', 'N26\\tReference T18 Wikidata:Q355\\tFacebook\\n', 'N27\\tReference T50 Wikidata:Q355\\tFacebook\\n', 'N28\\tReference T1 Wikidata:Q355\\tFacebook\\n', 'N29\\tReference T52 Wikidata:NULL\\tнет данных\\n', 'N30\\tReference T34 Wikidata:Q36215\\tМарк Цукерберг\\n', 'N31\\tReference T38 Wikidata:NULL\\tнет данных\\n', 'N32\\tReference T36 Wikidata:Q2563977\\t\\n', 'N33\\tReference T31 Wikidata:Q2563977\\t\\n', 'N34\\tReference T12 Wikidata:Q2563977\\t\\n', 'N35\\tReference T24 Wikidata:Q2563977\\t\\n', 'N36\\tReference T3 Wikidata:Q2563977\\t\\n', 'N37\\tReference T2 Wikidata:Q2563977\\t\\n']\n",
            "Offset:\n",
            "44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using annotation, generates a mask that contains 1s in the places corresponding to named entities and 0s otherwise\n",
        "def generate_NE_mask(text, ann, offset):\n",
        "    mask = [0] * len(text)\n",
        "    for line in ann:\n",
        "        if line[0] == 'T': # \"T\" - terms; in annotation, they correspond to named entites\n",
        "            if ';' in line:\n",
        "                line = line.split(';')[0]\n",
        "            start_ind, end_ind = line.split('\\t')[1].split(' ')[1:3]\n",
        "            #named_entity = line.split('\\t')[2].strip()\n",
        "            start_ind = int(start_ind) - offset\n",
        "            end_ind = int(end_ind) - offset\n",
        "            if start_ind >= 0:\n",
        "                for i in range(start_ind, end_ind):\n",
        "                    mask[i] = 1\n",
        "    return mask"
      ],
      "metadata": {
        "id": "RhD_XqKaHDB6"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = generate_NE_mask(text, ann, offset)\n",
        "for i in range(min(len(mask), 250)):\n",
        "    print(mask[i], end='')\n",
        "print()\n",
        "print(text[:min(len(mask), 250)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4HbwUUNswif",
        "outputId": "4994216a-205b-4643-f343-79133e6cc221"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1111111111111111111110000000000000000011111111011111111011111111101111111111111100111111111111110000000000001111111111111111111111100000000000000111111110000000000011111111110000000011111111000001111111111111111111110000000000000000000000000000011111\n",
            "Финансовым директором социальной сети Facebook назначен 39-летний Дэвид Эберсман (David Ebersman), сообщает The Wall Street Journal. На работу в Facebook он выйдет в сентябре. Ранее Эберсман был финансовым директором биотехнологической компании Genen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splits text into sentences remembering start positions of each one\n",
        "# (relative to text that does not contain name of the article, so, to match the positions in annotation, you need to add offset)\n",
        "def split_text_into_sentences(text):\n",
        "    sentences = sent_tokenize(text, language=\"russian\")\n",
        "    sentence_start_positions = []\n",
        "    for i in range(len(sentences)):\n",
        "        sentence_start_positions.append(text.find(sentences[i]))\n",
        "    return sentences, sentence_start_positions"
      ],
      "metadata": {
        "id": "oLro7kTQ1hBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, sentence_start_positions = split_text_into_sentences(text)\n",
        "for i in range(len(sentences)):\n",
        "    print(sentence_start_positions[i], sentences[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKkdV_out1lw",
        "outputId": "289d8704-e91f-46bb-b6b0-fa177d44d9c0"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Финансовым директором социальной сети Facebook назначен 39-летний Дэвид Эберсман (David Ebersman), сообщает The Wall Street Journal.\n",
            "133 На работу в Facebook он выйдет в сентябре.\n",
            "176 Ранее Эберсман был финансовым директором биотехнологической компании Genentech.\n",
            "257 Эберсман подчеркнул, что видит много общего между Facebook и Genentech.\n",
            "329 В частности, это две быстрорастущие компании с сильной корпоративной культурой.\n",
            "409 Также он заявил, что Facebook ожидает 70-процентное увеличение выручки в 2009 году.\n",
            "494 В компании Genentech Дэвид Эберсман проработал 15 лет.\n",
            "549 Ее финансовым директором он стал в 2006 году.\n",
            "595 На этой должности Эберсман проработал до апреля 2009 года, когда Roche Holding купил Genentech.\n",
            "692 По данным The Wall Street Journal, на должность финансового директора Facebook претендовали 11 кандидатов.\n",
            "799 Каждый из них прошел собеседование с основателем Facebook Марком Цукербергом (Mark Zuckerberg) и другими руководителями компании.\n",
            "930 Ранее финансовым директором Facebook был Гидеон Ю (Gideon Yu).\n",
            "993 Он покинул компанию три месяца назад.\n",
            "1031 Вместо него социальная сеть решила нанять топ-менеджера с опытом работы в публичной компании.\n",
            "1126 Facebook является крупнейшей западной социальной сетью.\n",
            "1182 За пять лет существования этого проекта его аудитория превысила 200 миллионов человек.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splits sentences into words remembering start position of each one (relative to text without the name of article)\n",
        "# gets rid of punctuation and stop words while doing so\n",
        "def split_sentences_into_words(text, sentences, sentence_start_positions):\n",
        "\n",
        "    words = []\n",
        "    words_start_positions = []\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        current_words = word_tokenize(sentences[i], language=\"russian\")\n",
        "        stop = set(stopwords.words('russian') + list(string.punctuation))\n",
        "        current_words = [word for word in current_words if word not in stop]\n",
        "\n",
        "        current_piece_of_text = text[sentence_start_positions[i]:]\n",
        "        current_words_start_positions = []\n",
        "        for word in current_words:\n",
        "            current_words_start_positions.append(current_piece_of_text.find(word) + sentence_start_positions[i])\n",
        "        words.append(current_words)\n",
        "        words_start_positions.append(current_words_start_positions)\n",
        "\n",
        "    return words, words_start_positions"
      ],
      "metadata": {
        "id": "R099T_M0d7sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words, words_start_positions = split_sentences_into_words(text, sentences, sentence_start_positions)\n",
        "print(words)\n",
        "print(words_start_positions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4wcbl9Fu5Gr",
        "outputId": "2de5b236-5693-4c65-f1c5-a90b723605ec"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Финансовым', 'директором', 'социальной', 'сети', 'Facebook', 'назначен', '39-летний', 'Дэвид', 'Эберсман', 'David', 'Ebersman', 'сообщает', 'The', 'Wall', 'Street', 'Journal'], ['На', 'работу', 'Facebook', 'выйдет', 'сентябре'], ['Ранее', 'Эберсман', 'финансовым', 'директором', 'биотехнологической', 'компании', 'Genentech'], ['Эберсман', 'подчеркнул', 'видит', 'общего', 'Facebook', 'Genentech'], ['В', 'частности', 'это', 'две', 'быстрорастущие', 'компании', 'сильной', 'корпоративной', 'культурой'], ['Также', 'заявил', 'Facebook', 'ожидает', '70-процентное', 'увеличение', 'выручки', '2009', 'году'], ['В', 'компании', 'Genentech', 'Дэвид', 'Эберсман', 'проработал', '15', 'лет'], ['Ее', 'финансовым', 'директором', 'стал', '2006', 'году'], ['На', 'должности', 'Эберсман', 'проработал', 'апреля', '2009', 'года', 'Roche', 'Holding', 'купил', 'Genentech'], ['По', 'данным', 'The', 'Wall', 'Street', 'Journal', 'должность', 'финансового', 'директора', 'Facebook', 'претендовали', '11', 'кандидатов'], ['Каждый', 'прошел', 'собеседование', 'основателем', 'Facebook', 'Марком', 'Цукербергом', 'Mark', 'Zuckerberg', 'другими', 'руководителями', 'компании'], ['Ранее', 'финансовым', 'директором', 'Facebook', 'Гидеон', 'Ю', 'Gideon', 'Yu'], ['Он', 'покинул', 'компанию', 'месяца', 'назад'], ['Вместо', 'социальная', 'сеть', 'решила', 'нанять', 'топ-менеджера', 'опытом', 'работы', 'публичной', 'компании'], ['Facebook', 'является', 'крупнейшей', 'западной', 'социальной', 'сетью'], ['За', 'пять', 'лет', 'существования', 'проекта', 'аудитория', 'превысила', '200', 'миллионов', 'человек']]\n",
            "[[0, 11, 22, 33, 38, 47, 56, 66, 72, 82, 88, 99, 108, 112, 117, 124], [133, 136, 145, 157, 166], [176, 182, 195, 206, 217, 236, 245], [257, 266, 282, 294, 307, 318], [329, 331, 342, 346, 350, 365, 376, 384, 398], [409, 418, 430, 439, 447, 461, 472, 482, 487], [494, 496, 505, 515, 521, 530, 541, 544], [549, 552, 563, 577, 584, 589], [595, 603, 613, 622, 636, 643, 648, 660, 666, 674, 680], [692, 695, 702, 706, 711, 718, 730, 740, 752, 762, 771, 784, 787], [799, 813, 820, 836, 848, 857, 864, 877, 882, 896, 904, 919], [930, 936, 947, 958, 971, 978, 981, 988], [993, 996, 1004, 1017, 1024], [1031, 1043, 1054, 1059, 1066, 1073, 1089, 1096, 1105, 1115], [1126, 1135, 1144, 1155, 1164, 1175], [1182, 1185, 1190, 1194, 1214, 1226, 1236, 1246, 1250, 1260]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_frequencies = pd.read_csv('/content/drive/My Drive/freqrnc2011.csv', sep='\\t', index_col='Lemma')\n",
        "\n",
        "len_threshold = 7\n",
        "frequency_threshold = 1/21\n",
        "\n",
        "def word_complexity(word, method):\n",
        "    if method == \"len\":\n",
        "        return len(word)\n",
        "    if method == \"frequency\":\n",
        "        if word in word_frequencies.index:\n",
        "            word_frequency = word_frequencies.loc[word]['Freq(ipm)']\n",
        "            if isinstance(word_frequency, pd.core.series.Series):\n",
        "                word_frequency = max(word_frequency)\n",
        "            return 1/word_frequency\n",
        "        else:\n",
        "            return 1/20\n",
        "#            return 15/word_frequency + len(word)/14\n",
        "#        else:\n",
        "#            return len(word)/7"
      ],
      "metadata": {
        "id": "vpCnjnYmGDAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normal_form(word):\n",
        "    return morph.parse(word)[0].normal_form"
      ],
      "metadata": {
        "id": "W9DPeBUt6DsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_simplifier = pipeline(\"fill-mask\", model=\"bert-base-multilingual-cased\")\n",
        "\n",
        "def bert_model(initial_sentence, masked_word, word_start_position_in_initial_sentence, complexity_method):#, word_start_position_in_simplified_sentence, simplified_sentence):\n",
        "    masked_sentence = initial_sentence[:word_start_position_in_initial_sentence] + \"[MASK]\" + initial_sentence[word_start_position_in_initial_sentence + len(masked_word):]\n",
        "    results = bert_simplifier(masked_sentence)\n",
        "    for result in results:\n",
        "        suggested_word = result['token_str']\n",
        "        if word_complexity(get_normal_form(suggested_word), complexity_method) < word_complexity(masked_word, complexity_method):\n",
        "            return suggested_word\n",
        "    return masked_word "
      ],
      "metadata": {
        "id": "3VhrLCOPmFPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2047a6ff-479c-4d84-97e5-3fd6bfbd5db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms(word):\n",
        "    url = 'https://dictionary.yandex.net/api/v1/dicservice.json/lookup'\n",
        "    params = {\n",
        "        'key': 'dict.1.1.20230528T133010Z.158a1e5336183ca0.4e03e68250b1dfaa01b77acbf3439b0e45d1a614',\n",
        "        'lang': 'ru-ru',\n",
        "        'text': word\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    data = json.loads(response.text)\n",
        "    try:\n",
        "        synonyms_list = []\n",
        "        for i in data['def'][0]['tr']:\n",
        "            for j in i['syn']:\n",
        "                synonyms_list.append(j['text'])\n",
        "            synonyms_list.append(i['text'])\n",
        "        return synonyms_list\n",
        "    except:\n",
        "        return []"
      ],
      "metadata": {
        "id": "No9gCzS2Lppa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_declension(word):\n",
        "    p = morph.parse(word)[0]\n",
        "    tags = set()\n",
        "    for tag in {p.tag.case, p.tag.gender, p.tag.number, p.tag.tense, p.tag.person, p.tag.aspect, p.tag.involvement, p.tag.mood, p.tag.voice}:\n",
        "        if tag != None:\n",
        "            tags.add(tag)\n",
        "    return tags"
      ],
      "metadata": {
        "id": "9ByDwh1naNXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_declension(\"красноречивый\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZlxSjFMJZFU",
        "outputId": "9bbbd24b-07f2-49fa-b1a1-6212c397e6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'masc', 'nomn', 'sing'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decline(word, tags):\n",
        "    p = morph.parse(word)[0]\n",
        "    for tag in tags:\n",
        "        if p.inflect({tag}) != None:\n",
        "            p = p.inflect({tag})\n",
        "    return p.word"
      ],
      "metadata": {
        "id": "aOY-TT3ceZ70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def synonym_model(word, complexity_method):\n",
        "    declension = get_declension(word)\n",
        "    normalized_word = get_normal_form(word)\n",
        "    synonyms = get_synonyms(normalized_word)\n",
        "    #print(word)\n",
        "    #print(synonyms)\n",
        "    result = word\n",
        "    for synonym in synonyms:\n",
        "        if word_complexity(synonym, complexity_method) < word_complexity(result, complexity_method):\n",
        "            result = synonym\n",
        "    result = decline(result, declension)\n",
        "    if word[0].isupper():\n",
        "        result = result.capitalize()\n",
        "    return result"
      ],
      "metadata": {
        "id": "8688XPB0Pt29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synonym_model(\"апробации\", \"frequency\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Xb7yfzGeUxZ-",
        "outputId": "bb9bdcc9-0c62-4787-b98b-664c65b55e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'испытания'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simple_dir = '/content/drive/My Drive/simplifications'\n",
        "if not os.path.exists(os.path.join(simple_dir, 'original')):\n",
        "    os.makedirs(os.path.join(simple_dir, 'original'))\n",
        "\n",
        "def write_list_to_file(list, path): # path where the file is located\n",
        "    with open(path, 'w') as f:\n",
        "        for item in list:\n",
        "            f.write(\"%s\\n\" % item)\n",
        "\n",
        "def read_list_from_file(path):\n",
        "    with open(path) as f:\n",
        "        sentences = f.readlines()\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "CRzr6BppIW2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simplify(text_location, model, complexity_method):\n",
        "\n",
        "    text, ann, offset = read_text(text_location)\n",
        "    mask = generate_NE_mask(text, ann, offset)\n",
        "    sentences, sentence_start_positions = split_text_into_sentences(text)\n",
        "    words, words_start_positions = split_sentences_into_words(text, sentences, sentence_start_positions)\n",
        "\n",
        "    file_name = text_location.split('/')[-1]\n",
        "    if not os.path.exists(os.path.join(simple_dir, 'original', file_name)):\n",
        "        write_list_to_file(sentences, os.path.join(simple_dir, 'original', file_name))\n",
        "    simplified_sentences = []\n",
        "    \n",
        "    for sentence_ind in range(len(sentences)):\n",
        "        if model == \"baseline\":\n",
        "            simplified_sentence = base_model(sentences[sentence_ind])\n",
        "        else:\n",
        "            simplified_sentence = sentences[sentence_ind]\n",
        "            words_start_positions_in_simplified_sentence = words_start_positions[sentence_ind][:]\n",
        "            for word_ind in range(len(words[sentence_ind])):\n",
        "                words_start_positions_in_simplified_sentence[word_ind] -= sentence_start_positions[sentence_ind]\n",
        "\n",
        "            for word_ind in range(len(words[sentence_ind])):\n",
        "                current_word = words[sentence_ind][word_ind]\n",
        "                if mask[words_start_positions[sentence_ind][word_ind]] == 0: # if the current word is not part of a named entity\n",
        "                    normalized_current_word = get_normal_form(current_word)\n",
        "                    if complexity_method == \"len\":\n",
        "                        threshold = len_threshold\n",
        "                    else:\n",
        "                        threshold = frequency_threshold\n",
        "                    if word_complexity(normalized_current_word, complexity_method) > threshold:\n",
        "                        current_word_start_position_in_initial_sentence = words_start_positions[sentence_ind][word_ind] - sentence_start_positions[sentence_ind]\n",
        "                        current_word_start_position_in_simplified_sentence = words_start_positions_in_simplified_sentence[word_ind]\n",
        "                        #print(current_word)\n",
        "                        suggested_word = current_word\n",
        "                        if model == \"bert\":\n",
        "                            suggested_word = bert_model(sentences[sentence_ind], current_word, current_word_start_position_in_initial_sentence, complexity_method)\n",
        "                        elif model == \"synonym\":\n",
        "                            suggested_word = synonym_model(current_word, complexity_method)\n",
        "\n",
        "                        #print(suggested_word)\n",
        "                        simplified_sentence = simplified_sentence[:current_word_start_position_in_simplified_sentence] + suggested_word + simplified_sentence[current_word_start_position_in_simplified_sentence + len(current_word):]\n",
        "                        simplification_offset = len(suggested_word) - len(current_word)\n",
        "\n",
        "                        for new_word_ind in range(word_ind + 1, len(words[sentence_ind])):\n",
        "                            words_start_positions_in_simplified_sentence[new_word_ind] += simplification_offset\n",
        "\n",
        "        simplified_sentences.append(simplified_sentence)\n",
        "        print(sentences[sentence_ind])\n",
        "        print(simplified_sentence)\n",
        "        print()\n",
        "\n",
        "    return simplified_sentences"
      ],
      "metadata": {
        "id": "J8Q5bKENiGMl"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(simplify(text_location=\"/tmp/NEREL-master/NEREL-v1.1/train/023.txt\", model=\"synonym\", complexity_method=\"len\"))"
      ],
      "metadata": {
        "id": "QI8lXX5zuOKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad006b1-0f36-4066-c758-10c96c0c4c70"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Финансовым директором социальной сети Facebook назначен 39-летний Дэвид Эберсман (David Ebersman), сообщает The Wall Street Journal.\n",
            "Финансовым директором светской сети Facebook назначен 39-летний Дэвид Эберсман (David Ebersman), пишет The Wall Street Journal.\n",
            "\n",
            "На работу в Facebook он выйдет в сентябре.\n",
            "На работу в Facebook он выйдет в сентябре.\n",
            "\n",
            "Ранее Эберсман был финансовым директором биотехнологической компании Genentech.\n",
            "Ранее Эберсман был финансовым директором биотехнологической компании Genentech.\n",
            "\n",
            "Эберсман подчеркнул, что видит много общего между Facebook и Genentech.\n",
            "Эберсман указал, что видит много общего между Facebook и Genentech.\n",
            "\n",
            "В частности, это две быстрорастущие компании с сильной корпоративной культурой.\n",
            "В частности, это две быстрорастущие компании с сильной общей культурой.\n",
            "\n",
            "Также он заявил, что Facebook ожидает 70-процентное увеличение выручки в 2009 году.\n",
            "Также он заявил, что Facebook ожидает 70-процентное увеличение выручки в 2009 году.\n",
            "\n",
            "В компании Genentech Дэвид Эберсман проработал 15 лет.\n",
            "В компании Genentech Дэвид Эберсман прошёл 15 лет.\n",
            "\n",
            "Ее финансовым директором он стал в 2006 году.\n",
            "Ее финансовым директором он стал в 2006 году.\n",
            "\n",
            "На этой должности Эберсман проработал до апреля 2009 года, когда Roche Holding купил Genentech.\n",
            "На этой должности Эберсман прошёл до апреля 2009 года, когда Roche Holding купил Genentech.\n",
            "\n",
            "По данным The Wall Street Journal, на должность финансового директора Facebook претендовали 11 кандидатов.\n",
            "По данным The Wall Street Journal, на должность финансового директора Facebook обижались 11 соперников.\n",
            "\n",
            "Каждый из них прошел собеседование с основателем Facebook Марком Цукербергом (Mark Zuckerberg) и другими руководителями компании.\n",
            "Каждый из них прошел спор с отцом Facebook Марком Цукербергом (Mark Zuckerberg) и другими руководителями компании.\n",
            "\n",
            "Ранее финансовым директором Facebook был Гидеон Ю (Gideon Yu).\n",
            "Ранее финансовым директором Facebook был Гидеон Ю (Gideon Yu).\n",
            "\n",
            "Он покинул компанию три месяца назад.\n",
            "Он покинул компанию три месяца назад.\n",
            "\n",
            "Вместо него социальная сеть решила нанять топ-менеджера с опытом работы в публичной компании.\n",
            "Вместо него светская сеть решила нанять топ-менеджера с опытом работы в публичной компании.\n",
            "\n",
            "Facebook является крупнейшей западной социальной сетью.\n",
            "Facebook есть крупнейшей западной светской сетью.\n",
            "\n",
            "За пять лет существования этого проекта его аудитория превысила 200 миллионов человек.\n",
            "За пять лет века этого проекта его зал перешла 200 миллионов человек.\n",
            "\n",
            "['Финансовым директором светской сети Facebook назначен 39-летний Дэвид Эберсман (David Ebersman), пишет The Wall Street Journal.', 'На работу в Facebook он выйдет в сентябре.', 'Ранее Эберсман был финансовым директором биотехнологической компании Genentech.', 'Эберсман указал, что видит много общего между Facebook и Genentech.', 'В частности, это две быстрорастущие компании с сильной общей культурой.', 'Также он заявил, что Facebook ожидает 70-процентное увеличение выручки в 2009 году.', 'В компании Genentech Дэвид Эберсман прошёл 15 лет.', 'Ее финансовым директором он стал в 2006 году.', 'На этой должности Эберсман прошёл до апреля 2009 года, когда Roche Holding купил Genentech.', 'По данным The Wall Street Journal, на должность финансового директора Facebook обижались 11 соперников.', 'Каждый из них прошел спор с отцом Facebook Марком Цукербергом (Mark Zuckerberg) и другими руководителями компании.', 'Ранее финансовым директором Facebook был Гидеон Ю (Gideon Yu).', 'Он покинул компанию три месяца назад.', 'Вместо него светская сеть решила нанять топ-менеджера с опытом работы в публичной компании.', 'Facebook есть крупнейшей западной светской сетью.', 'За пять лет века этого проекта его зал перешла 200 миллионов человек.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assessment"
      ],
      "metadata": {
        "id": "PxYfwi8Thk2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/tmp/NEREL-master/NEREL-v1.1/train/\"\n",
        "file_list = os.listdir(directory)\n",
        "txt_file_list = [file_name for file_name in file_list if file_name.endswith(\".txt\")]\n",
        "\n",
        "random.shuffle(txt_file_list)\n",
        "\n",
        "sample_size = 20\n",
        "txt_file_list = txt_file_list[:sample_size]\n",
        "\n",
        "simple_dir = '/content/drive/My Drive/simplifications'\n",
        "models = [(\"baseline\", \"\"), (\"bert\", \"len\"), (\"bert\", \"frequency\"), (\"synonym\", \"len\"), (\"synonym\", \"frequency\")]"
      ],
      "metadata": {
        "id": "Gha4H8pjBsfG"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(txt_file_list) #['24632_text.txt']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdrQNH70Yq4n",
        "outputId": "8a3f7d60-1880-403b-b90c-ed2eeb1ee0ba"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['19054_text.txt', '12522_text.txt', '100986_text.txt', '39538_text.txt', '463.txt', '37719_text.txt', '20160_text.txt', '82299_text.txt', '149704_text.txt', '25896_text.txt', '105566_text.txt', '509121_text.txt', '38217_text.txt', '19358_text.txt', '133604_text.txt', '141316_text.txt', '25518_text.txt', '141309_text.txt', '531.txt', '24647_text.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(os.path.join(simple_dir, 'original')):\n",
        "    os.makedirs(os.path.join(simple_dir, 'original'))\n",
        "\n",
        "for file_name in txt_file_list:\n",
        "    for model, complexity_method in models:\n",
        "        file_path = os.path.join(directory, file_name)\n",
        "        if os.path.isfile(file_path):\n",
        "            simplified_sentences = simplify(file_path, model, complexity_method)\n",
        "            if not os.path.exists(os.path.join(simple_dir, model, complexity_method)):\n",
        "                os.makedirs(os.path.join(simple_dir, model, complexity_method))\n",
        "            write_list_to_file(simplified_sentences, os.path.join(simple_dir, model, complexity_method, file_name))"
      ],
      "metadata": {
        "id": "1PJ0M4Tt_q9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_syllables(word):\n",
        "    word = re.sub(r'[^аеёиоуыэюя]', '', word.lower())\n",
        "    return len(word)\n",
        "\n",
        "def asw(sentences): # average number of syllables in the word\n",
        "    total_avg_syllables = 0\n",
        "    total_sentences = 0\n",
        "    for sentence in sentences:\n",
        "        words = re.findall(r'\\b\\w+\\b', sentence)\n",
        "        if len(words) != 0:\n",
        "            total_sentences += 1\n",
        "            total_syllables = 0\n",
        "            for word in words:\n",
        "                total_syllables += count_syllables(word)\n",
        "            total_avg_syllables += total_syllables / len(words)\n",
        "    return total_avg_syllables / total_sentences"
      ],
      "metadata": {
        "id": "tLxw0k_P-hRz"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_embed_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_embed_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def bert_cos_dist(original_sentences, simple_sentences):\n",
        "    avg_dist = 0\n",
        "    for i in range(min(len(simple_sentences), len(original_sentences))):\n",
        "        sentence1 = original_sentences[i]\n",
        "        sentence2 = simple_sentences[i]\n",
        "\n",
        "        encoded_input = bert_embed_tokenizer([sentence1, sentence2], padding=True, truncation=True, return_tensors='pt')\n",
        "        with torch.no_grad():\n",
        "            output = bert_embed_model(**encoded_input)\n",
        "\n",
        "        sentence1_embedding = output.last_hidden_state[0, 0, :]\n",
        "        sentence2_embedding = output.last_hidden_state[1, 0, :]\n",
        "\n",
        "        dist = cosine_distances(sentence1_embedding.reshape(1, -1), sentence2_embedding.reshape(1, -1)).item()\n",
        "        avg_dist += dist    \n",
        "    return avg_dist / min(len(simple_sentences), len(original_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8lpTDunJtNz",
        "outputId": "1015682d-7496-4418-e978-7b7bef5b8ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences1 = [\"Мне нравятся коты\", \"I love you\"]\n",
        "sentences2 = [\"Я люблю собак\", \"Je t'aime\"]\n",
        "distance = bert_cos_dist(sentences1, sentences2)\n",
        "print(\"Cosine distance:\", distance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHUJv2RMJ1Ka",
        "outputId": "8dd30e47-5642-4637-d14f-dc00674d6370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine distance: 0.0805254876613617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def assess_model(simple_dir, model, file_list, metric):\n",
        "    avg_metric = 0\n",
        "    files_num = 0\n",
        "    for file_name in file_list:\n",
        "        simple_location = os.path.join(simple_dir, model[0], model[1], file_name)\n",
        "        simple_sentences = read_list_from_file(simple_location)\n",
        "        if metric == 'asw':\n",
        "            if len(simple_sentences) != 0:\n",
        "                files_num += 1\n",
        "                avg_metric += asw(simple_sentences)\n",
        "        elif metric == 'bert_cos_dist':\n",
        "            original_location = os.path.join(simple_dir, 'original', file_name)\n",
        "            original_sentences = read_list_from_file(original_location)\n",
        "            print(simple_sentences)\n",
        "            print(original_sentences)\n",
        "            avg_metric += bert_cos_dist(simple_sentences, original_sentences)\n",
        "    return avg_metric/files_num"
      ],
      "metadata": {
        "id": "kbK1TjL2BH6t"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in models:\n",
        "    for metric in ['asw']: #\"bert_cos_dist\"\n",
        "        res = assess_model(simple_dir, model, txt_file_list, metric)\n",
        "        print(\"Model: \" + str(model[0]))\n",
        "        print(\"Complexity method: \" + str(model[1]))\n",
        "        print(\"Metric: \" + metric)\n",
        "        print(\"Result: \" + str(res))\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DfOJLIjnFVB",
        "outputId": "5eeca989-4fd6-428e-d145-b6f6cb73afa0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: baseline\n",
            "Complexity method: \n",
            "Metric: asw\n",
            "Result: 2.4154692467190717\n",
            "\n",
            "Model: bert\n",
            "Complexity method: len\n",
            "Metric: asw\n",
            "Result: 2.1144077338760026\n",
            "\n",
            "Model: bert\n",
            "Complexity method: frequency\n",
            "Metric: asw\n",
            "Result: 2.3621361878725375\n",
            "\n",
            "Model: synonym\n",
            "Complexity method: len\n",
            "Metric: asw\n",
            "Result: 2.2702403694104114\n",
            "\n",
            "Model: synonym\n",
            "Complexity method: frequency\n",
            "Metric: asw\n",
            "Result: 2.441953394574341\n",
            "\n"
          ]
        }
      ]
    }
  ]
}